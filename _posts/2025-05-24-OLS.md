---
title: "OLS"
date: 2025-05-24
permalink: /posts/2025/05/OLS/
tags:
  - math
  - linear_algebra
---

In this article I'll review some basics on linear regression (or to use a better term, *ordinary least squares* (OLS)), show several approaches to solving an OLS problem, and how these connect to ($L_{2}$) regularization techniques.

* What you already know:
  * Linear algebra: matrix operations, inner products, vector norm and distance.
    * The section on the convergence condition for gradient descent requires some more theory; in particular, symmetric and orthogonal matrices, diagonalization, and eigendecomposition.
* How this article might interest you
  * You already have some familiarity with linear regression but want a deeper, unified understanding of how this topic is introduced in textbooks belonging to different fields (linear algebra, statistics, etc.)
  * You have seen some rather intimidating formulas on linear regression and wonder where they come from!

# Preliminaries

Basically, we have an *inconsistent* system of equations $X\mathbf{w}\neq\mathbf{y}$. Here, $X$ is data matrix with $m$ rows (observations/samples) and $n$ columns (features/variables), $\mathbf{y}$ is the $m$-vector of dependent variables (labels), and $\mathbf{w}$ is the $n$-vector of our model's parameters. The model can be formulated as

{% raw %}
$$
\hat{y}_{i}=\sum_{i=1}^m w_{i}x_{ij}=\mathbf{w}^T\mathbf{x}_{i}
$$
{% endraw %}

Where $\hat{y}_{j}$ is the prediction for the $j$-th subject, $x_{ij}$ the $j$-th entry in the $i$-th column of $X$, and $\mathbf{x}_{i}$ the $i$-th row of $X$.

* I assume that $X$ already contains a column of $1$s, which, when multiplied by it corresponding parameter $w$ in $\mathbf{w}$, is equivalent to adding a bias term $b$ ($\mathbf{w}^T\mathbf{x}+b$). This greatly simplifies the formulas we’ll see later.

> [!disclaimer]- Disclaimer on Notation  
> There's a variety of letters used to describe the above problem:
> * Linear algebra books introduce systems of equations as $A\mathbf{x}=\mathbf{b}$ and least-squares problems as $A\mathbf{x} \neq \mathbf{b}$.
> * Statistics textbooks introduces the problem as $X\beta \neq \mathbf{y}$ and the solution as $X \hat{\beta}=\hat{\mathbf{y}}$, where $\hat{\beta}$ is the optimal set of parameters and $\hat{\mathbf{y}}$ is the vector of predictions made by the optimal model; also, $X$ is known as the *design matrix*.
> * Machine learning literature denotes the parameters of the model as *weights*, therefore the problem is written as $X \mathbf{w} \neq \mathbf{y}$.

Since we can't find a vector $\mathbf{w}$ that satisfies $X\mathbf{w}=\mathbf{y}$, we're instead in minimizing the (Euclidean) distance between $X\mathbf{w}$ and $\mathbf{y}$:

{% raw %}
$$
\lVert X\mathbf{w} - \mathbf{y} \rVert
  = \sqrt{ (X\mathbf{w}-\mathbf{y})^T (X\mathbf{w}-\mathbf{y}) }
  = \sqrt{ \sum_{i=1}^m (\mathbf{w}^T \mathbf{x}_{i} - y_{i})^2 }
$$
{% endraw %}

The above expression is routinely squared for convenience, known as the *sum of squared errors* (we'll see some better arguments for squaring the errors later). Minimizing $\sum_{i=1}^m (\mathbf{w}^T\mathbf{x}_{i}-y_{i})^2$ is the same as minimizing $\frac{1}{m}\sum_{i=1}^m (\mathbf{w}^T\mathbf{x}_{i}-y_{i})^2$, which is just the *mean square error*, a well-known performance metric for regression models.

Formally, we’re interested in:

{% raw %}
$$
\mathop{\mathrm{argmin}}_{\mathbf{w}}
  \lVert X\mathbf{w} - \mathbf{y} \rVert^2
$$
{% endraw %}

* Going forward, I’ll be assuming the columns (features) in $X$ are linearly independent. This assumption guarantees the *uniqueness* of the optimal solution to the problem, denoted $\hat{\mathbf{w}}$, and also the invertibility of the Gram matrix $X^TX$.
* In machine learning terms, we can consider $\lVert X\mathbf{w}-\mathbf{y} \rVert^2$ as our **loss/cost function**, denoted $J$. Optimization literature refers to $\lVert X \mathbf{w} - \mathbf{y} \rVert$ as the **objective function**.

# Perspective 1: Closed form solution via calculus

The optimal solution to an OLS (which is a minimizer of $f(\mathbf{w}) = \lVert X\mathbf{w}-\mathbf{y}\rVert^2$) must satisfy

{% raw %}
$$
\frac{\partial f(\hat{\mathbf{w}})}{\partial w_{i}} = 0
\quad i = 1,\dots,n
$$
{% endraw %}

… and so on for every subsequent display-math block in your file. Just wrap each:

```markdown
{% raw %}
$$
…LaTeX…
$$
{% endraw %}
